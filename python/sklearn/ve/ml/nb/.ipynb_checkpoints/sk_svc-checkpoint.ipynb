{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Scikit Learn Tutorial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tables\n",
    "import sklearn as sk\n",
    "from sklearn import tree, svm, metrics\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Prettier plots\n",
    "def getfig (fignum=None, aspect=None, width=None, figsize=None):\n",
    "    aspect = aspect or 4/3.\n",
    "    width = width or 7\n",
    "    if figsize is None:\n",
    "        figsize = (width, width / aspect)\n",
    "    out = plt.figure (num=fignum, figsize=figsize)\n",
    "    plt.clf ()\n",
    "    return out\n",
    "\n",
    "def pfig (*a, **kw):\n",
    "    fig = getfig (*a, **kw)\n",
    "    ax = fig.add_subplot (111)\n",
    "    return fig, ax\n",
    "\n",
    "def histpoints(data,bins):\n",
    "    counts,bin_edges = np.histogram(data,bins)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:])/2.\n",
    "    return bin_centers, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sig test data:     134 cols, 710948 rows\n",
      "sig training data: 134 cols, 710948 rows\n",
      "test data:      123 cols, 3884472 rows\n",
      "training data:  123 cols, 3884472 rows\n",
      "bg test data:      132 cols, 21765 rows\n",
      "bg training data:  132 cols, 21765 rows\n"
     ]
    }
   ],
   "source": [
    "#import necessary data\n",
    "datafolder = '/home/relethford/git/practice/python/sklearn/ve/ml/data/nu_anis/2013/'\n",
    "\n",
    "#sig\n",
    "sig_test = tables.open_file(datafolder+'test_sig.ds.hdf5')\n",
    "sig_train = tables.open_file(datafolder+'train_sig.ds.hdf5')\n",
    "#bg\n",
    "data_test = tables.open_file(datafolder+'test_data.ds.hdf5')\n",
    "data_train = tables.open_file(datafolder+'train_data.ds.hdf5')\n",
    "#corsika\n",
    "bg_test = tables.open_file(datafolder+'test_corsika.ds.hdf5')\n",
    "bg_train = tables.open_file(datafolder+'train_corsika.ds.hdf5')\n",
    "\n",
    "#print shape of each set\n",
    "print('sig test data:     {} cols, {} rows'.format(len(sig_test.root.table.colnames),len(sig_test.root.table.cols.zen)))\n",
    "print('sig training data: {} cols, {} rows'.format(len(sig_train.root.table.colnames),len(sig_train.root.table.cols.zen)))\n",
    "print('test data:      {} cols, {} rows'.format(len(data_test.root.table.colnames),len(data_test.root.table.cols.zen)))\n",
    "print('training data:  {} cols, {} rows'.format(len(data_train.root.table.colnames),len(data_train.root.table.cols.zen)))\n",
    "print('bg test data:      {} cols, {} rows'.format(len(bg_test.root.table.colnames),len(bg_test.root.table.cols.zen)))\n",
    "print('bg training data:  {} cols, {} rows'.format(len(bg_train.root.table.colnames),len(bg_train.root.table.cols.zen)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's set up a fcn that grabs a given variable from a given dataset, but only out to N events.\n",
    "N = 10000\n",
    "def getValue(colNames, datafile):\n",
    "  #value = datafile.get_node('/','table').col(colName)[0:N]\n",
    "  return np.array(list(datafile.get_node('/','table').col(colName)[0:N] for colName in colNames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make an array for each of the for data sets, each having the variables we want to investigate.\n",
    "def varStack(tuple_vars):\n",
    "    return np.vstack(tuple_vars).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For now, we'll take two variables. Lizz says these two (bayes_rat, cog_z) have greatest separation power.\n",
    "#Add a target for each point - for bg, 0, for sig, 1.\n",
    "sig_test  = varStack((getValue(('bayes_rat','cog_z'),  sig_test),np.ones(N)))\n",
    "sig_train = varStack((getValue(('bayes_rat','cog_z'),  sig_train),np.ones(N)))\n",
    "bg_test   = varStack((getValue(('bayes_rat','cog_z'), bg_test),np.zeros(N)))\n",
    "bg_train  = varStack((getValue(('bayes_rat','cog_z'), bg_train),np.zeros(N)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#And also for nonMC test data for later use. For these we don't add a target point.\n",
    "data_test  = varStack((getValue(('bayes_rat','cog_z'), data_test)))\n",
    "#Note - THIS is the part where the code stalls like crazy. Either need to figure out how to only access SOME of the data...\n",
    "#...or else chop it up ahead of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  16.90712089  318.31675093    0.        ]\n",
      " [  16.45511196 -434.50173732    0.        ]\n",
      " [  19.46631814  149.39222895    0.        ]\n",
      " ..., \n",
      " [  18.04611691  137.6922138     0.        ]\n",
      " [  16.06739548  239.1964964     0.        ]\n",
      " [  17.86273514  234.13741212    0.        ]]\n",
      "(10000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(bg_test)\n",
    "print(np.shape(bg_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Combine the test and train arrays, since we want a mixture of bg and sig in our model here.\n",
    "test = np.concatenate((sig_test,bg_test))\n",
    "train = np.concatenate((sig_train,bg_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a classifier: a support vector classifier. This is what is suggested in the tutorial for a simple vector classifier.\n",
    "classifier = svm.SVC(gamma=0.001, probability=True)\n",
    "#Based on what I know about machine learning, gamma is a learning speed parameter for minimization. The higher it is, the faster the fit converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
       "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sklearn takes over from here to fit these features in order to predict which digit they are. We divide into a teaching and target sample.\n",
    "classifier.fit(train[:,0:2],train[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now we can use that fit to classify the expected target for the remaining events.\n",
    "expected = test[:,-1]\n",
    "predicted = classifier.predict(test[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n",
      "[ 0.  1.  1.  1.  1.  1.  1.  0.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "#It doesn't line up perfectly... but let's see how well it predicts over 10,000.\n",
    "print(expected[0:10])\n",
    "print(predicted[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for classifier SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma=0.001, kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.83      0.92      0.87     10000\n",
      "        1.0       0.91      0.81      0.86     10000\n",
      "\n",
      "avg / total       0.87      0.86      0.86     20000\n",
      "\n",
      "\n",
      "Confusion matrix:\n",
      "[[9184  816]\n",
      " [1889 8111]]\n"
     ]
    }
   ],
   "source": [
    "#Finally, let's measure how well this predicter works.\n",
    "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
    "      % (classifier, metrics.classification_report(expected, predicted)))\n",
    "print(\"Confusion matrix:\\n%s\" % metrics.confusion_matrix(expected, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sigtest_p = classifier.predict_proba(sig_test[:,0:2])\n",
    "bgtest_p = classifier.predict_proba(bg_test[:,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Also, we'll predict the outcome of the training data, too, to get a good baseline. We probabily should only do this on some of the training data - say, 10000.\n",
    "sigtrain_p = classifier.predict_proba(sig_train[0:10000,0:2])\n",
    "bgtrain_p = classifier.predict_proba(bg_train[0:10000,0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#And now we'll do it on the non-MC data.\n",
    "datatest_p = classifier.predict_proba(data_test[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MC probabilities\n",
    "probabilities = np.concatenate((sigtrain_p,bgtrain_p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.72468694e-05,   7.61828686e-03,   7.90201029e-03,\n",
       "         1.29910078e-05,   1.71019999e-05,   7.73969684e-01,\n",
       "         1.01316157e-01,   5.58944445e-01,   1.13092667e-01,\n",
       "         5.77611520e-06])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities[0:10,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Rudimentary plot for cog_z and bayes_rat vs. probabilities\n",
    "fig,ax = pfig()\n",
    "size=0.2\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(test[:,0], probabilities[:,0], label = 'bg', color = 'blue',s=size)\n",
    "plt.scatter(test[:,0], probabilities[:,1], label = 'sig', color = 'green',s=size)\n",
    "\n",
    "#plt.ylim(0,2)\n",
    "#plt.xlim(-1.,1.)100\n",
    "plt.xlabel(r'cog_z')\n",
    "plt.ylabe,0:2l(r'probability')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(test[:,1], probabilities[:,0], label = 'bg', color = 'blue',s=size)\n",
    "plt.scatter(test[:,1], probabilities[:,1], label = 'sig', color = 'green',s=size)\n",
    "plt.xlabel(r'bayes_rat')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2d plot. (Doesn't currently work)\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111,projection='3d')\n",
    "#plt.scatter(test[:,0],test[:,1],probabilities[:,0], label = 'bg', color = 'blue')\n",
    "#plt.scatter(test[:,0],test[:,1],probabilities[:,1], label = 'sig', color = 'green')\n",
    "#ax.set_xlabel(r'cog_z')\n",
    "#ax.set_ylabel(r'bayes_rat')\n",
    "#ax.set_zlabel('probability')\n",
    "#plt.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#No idea why the probabilities are so low, but whatever. Let's try to emulate a bdt plot that lizz showed me.\n",
    "#BDT definition can be found at: http://software.icecube.wisc.edu/documentation/projects/pybdt/man_bdt_intro.html\n",
    "#But I'm not using BDT, am I? I'm using predict_proba. So let's try to do predict_proba distribution hists from 0 to 1, from each group of data.\n",
    "\n",
    "#First thing is to separate the probabilities into classes of thing. It will likely be easier to do this by calculating\n",
    "#probabilities on separate test data to begin with - sig_p and bg_p are good places to start.\n",
    "size = 0.5\n",
    "bins = 50\n",
    "binrange = (0,1)\n",
    "fig_p,ax_p = pfig(figsize=(15,5))\n",
    "\n",
    "#Linear scale\n",
    "plt.subplot(1,2,1)\n",
    "\n",
    "#bgtest_mid = histpoints(bgtest_p[:,1], bins = bins)\n",
    "#plt.scatter(bgtest_mid[0],bgtest_mid[1], label = 'bg test', color = 'blue', s=size)\n",
    "\n",
    "plt.hist(bgtest_p[:,1], bins = bins, range = binrange, label = 'bg test', color = 'blue', histtype='step')\n",
    "plt.hist(sigtest_p[:,1], bins = bins, range = binrange, label = 'sig test', color = 'green', histtype='step')\n",
    "plt.hist(bgtrain_p[:,1], bins = bins, range = binrange, label = 'bg train', color = 'dodgerblue', histtype='step')\n",
    "plt.hist(sigtrain_p[:,1], bins = bins, range = binrange, label = 'sig train', color = 'springgreen', histtype='step')\n",
    "plt.hist(probabilities[:,1], bins = bins, range = binrange, label = 'total MC', color = 'purple', histtype='step')\n",
    "plt.hist(datatest_p[:,1],bins = bins, range = binrange, label = 'data test', color = 'k', histtype='step')\n",
    "plt.xlabel(r'probability')\n",
    "plt.ylabel(r'frequency')\n",
    "plt.xlim(0,1)\n",
    "plt.ylim(0)\n",
    "plt.legend(loc='upper right', fontsize = 'small')\n",
    "\n",
    "#logarithmic scale\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(bgtest_p[:,1], bins = bins, range = binrange, label = 'bg test', color = 'blue', histtype='step')\n",
    "plt.hist(sigtest_p[:,1], bins = bins, range = binrange, label = 'sig test', color = 'green', histtype='step')\n",
    "plt.hist(bgtrain_p[:,1], bins = bins, range = binrange, label = 'bg train', color = 'dodgerblue', histtype='step')\n",
    "plt.hist(sigtrain_p[:,1], bins = bins, range = binrange, label = 'sig train', color = 'springgreen', histtype='step')\n",
    "plt.hist(probabilities[:,1], bins = bins, range = binrange, label = 'total MC', color = 'purple', histtype='step')\n",
    "plt.hist(datatest_p[:,1],bins = bins, range = binrange, label = 'data test', color = 'k', histtype='step')\n",
    "plt.xlabel(r'probability')\n",
    "plt.ylabel(r'frequency')\n",
    "plt.semilogy()\n",
    "plt.xlim(0,1)\n",
    "plt.legend(loc='upper right', fontsize = 'small')\n",
    "\n",
    "fig_p.savefig('plots/prob_distribution.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
